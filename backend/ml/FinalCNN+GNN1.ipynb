{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGa0tj6UhC3g",
        "outputId": "c4dad9fe-496f-4622-91dc-196a1265c20b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wfdb\n",
            "  Downloading wfdb-4.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.13.2)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2025.3.0)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.0.2)\n",
            "Collecting pandas>=2.2.3 (from wfdb)\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (1.16.3)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (11.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2025.11.12)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp>=3.10.11->wfdb) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n",
            "Downloading wfdb-4.3.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas, wfdb, torch_geometric\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.3.3 torch_geometric-2.7.0 wfdb-4.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wfdb torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX6rSU6TyuSw",
        "outputId": "38aa2b20-a81d-449b-97e8-bbb7f079ccd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting iterative-stratification\n",
            "  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.12/dist-packages (4.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from iterative-stratification) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from iterative-stratification) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from iterative-stratification) (1.6.1)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.13.2)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2025.3.0)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.3.3)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.32.4)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2025.11.12)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->iterative-stratification) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->iterative-stratification) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp>=3.10.11->wfdb) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n",
            "Downloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.9\n"
          ]
        }
      ],
      "source": [
        "!pip install iterative-stratification wfdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX7F57QPv9Hf",
        "outputId": "b5343567-e3e6-4c36-b1db-977c4cd66b08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.11.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qrZLfr6tK_o",
        "outputId": "98064170-dd71-4e8f-d66d-e487fff3ceef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAUkMcz8rgHW",
        "outputId": "610989b2-015a-453d-d9f5-0402e2cf77c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,849 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,539 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,623 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,598 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,961 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,410 kB]\n",
            "Fetched 28.4 MB in 3s (8,830 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "1 package can be upgraded. Run 'apt list --upgradable' to see it.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt update\n",
        "!apt install -y tesseract-ocr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiinuVZQrk2V",
        "outputId": "df2a02be-526c-4b94-d6a2-017792e916da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytesseract\n",
            "  Using cached pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (11.3.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ],
      "source": [
        "!pip install pytesseract\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w5OLf9XfsGZs",
        "outputId": "d3181ba8-f8f9-4d7c-b499-dd70f2a7c47a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: sympy 1.14.0\n",
            "Uninstalling sympy-1.14.0:\n",
            "  Successfully uninstalled sympy-1.14.0\n",
            "Found existing installation: torchvision 0.24.0+cu126\n",
            "Uninstalling torchvision-0.24.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.24.0+cu126\n",
            "Found existing installation: torch 2.9.0+cu126\n",
            "Uninstalling torch-2.9.0+cu126:\n",
            "  Successfully uninstalled torch-2.9.0+cu126\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m780.4/780.4 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m157.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m226.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m417.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m285.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m220.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m164.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m202.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m212.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m223.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m267.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m335.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m308.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m231.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m180.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Installing collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.9.0+cu126\n",
            "    Uninstalling torchaudio-2.9.0+cu126:\n",
            "      Successfully uninstalled torchaudio-2.9.0+cu126\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "3b28215df41c4c1cb139ecb12393a8bb",
              "pip_warning": {
                "packages": [
                  "sympy",
                  "torch",
                  "torchgen"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sympy==1.12\n",
            "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.12/dist-packages (from sympy==1.12) (1.3.0)\n",
            "Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/5.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/5.7 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# üîß ENVIRONMENT FIX (RUN ONCE)\n",
        "# ============================================================\n",
        "\n",
        "!pip uninstall -y sympy torchvision torch\n",
        "!pip install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install --no-cache-dir sympy==1.12\n",
        "!pip install --no-cache-dir torch-geometric\n",
        "\n",
        "print(\"‚úÖ Reinstall complete. PLEASE RESTART RUNTIME NOW.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxZ0HpOIbBP-",
        "outputId": "aafa4ff0-7708-4f9d-aa77-d9494a810592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch: 2.5.1+cu121\n",
            "Torchvision: 0.20.1+cu121\n",
            "Sympy: 1.12\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import sympy\n",
        "import torchvision\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Torchvision:\", torchvision.__version__)\n",
        "print(\"Sympy:\", sympy.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-TYyHKhwemp",
        "outputId": "5a3285b8-fab6-42df-972c-df21aaf5b8f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total PTB-XL ECGs containing at least one target label: 21388\n",
            "\n",
            "==============================\n",
            " LABEL COUNTS (MULTI-LABEL)\n",
            "==============================\n",
            "NORM  ‚Üí 9514\n",
            "MI    ‚Üí 5469\n",
            "HYP   ‚Üí 2649\n",
            "STTC  ‚Üí 5235\n",
            "CD    ‚Üí 4898\n",
            "\n",
            "==============================\n",
            " MULTI-LABEL DISTRIBUTION\n",
            "==============================\n",
            "2-label samples: 4068\n",
            "3-label samples: 919\n",
            "4-label samples: 157\n",
            "5-label samples: 0\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# COUNT MULTI-LABEL SAMPLES IN PTB-XL FOR 5 TARGET CLASSES\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "PTB = \"/content/drive/MyDrive/PTB Dataset\"\n",
        "\n",
        "# -------------------------------\n",
        "# Load metadata\n",
        "# -------------------------------\n",
        "df = pd.read_csv(f\"{PTB}/ptbxl_database.csv\")\n",
        "scp = pd.read_csv(f\"{PTB}/scp_statements.csv\", index_col=0)\n",
        "\n",
        "# -------------------------------\n",
        "# Extract diagnostic classes per ECG\n",
        "# -------------------------------\n",
        "def extract_labels(scp_dict):\n",
        "    d = eval(scp_dict)\n",
        "    labels = set()\n",
        "    for code in d.keys():\n",
        "        if code in scp.index:\n",
        "            labels.add(scp.loc[code].diagnostic_class)\n",
        "    return list(labels)\n",
        "\n",
        "df[\"labels\"] = df[\"scp_codes\"].apply(extract_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Only keep ECGs containing any of the 5 target labels\n",
        "# -------------------------------\n",
        "TARGETS = [\"NORM\", \"MI\", \"HYP\", \"STTC\", \"CD\"]\n",
        "\n",
        "def has_target(label_list):\n",
        "    return any(lbl in TARGETS for lbl in label_list)\n",
        "\n",
        "df = df[df[\"labels\"].apply(has_target)].reset_index(drop=True)\n",
        "\n",
        "print(\"Total PTB-XL ECGs containing at least one target label:\", len(df))\n",
        "\n",
        "# -------------------------------\n",
        "# Multi-hot encoding\n",
        "# -------------------------------\n",
        "def to_multihot(lbl_list):\n",
        "    arr = np.zeros(len(TARGETS))\n",
        "    for i, t in enumerate(TARGETS):\n",
        "        if t in lbl_list:\n",
        "            arr[i] = 1\n",
        "    return arr\n",
        "\n",
        "multihot = np.vstack(df[\"labels\"].apply(to_multihot))\n",
        "\n",
        "# -------------------------------\n",
        "# Count occurrences per label\n",
        "# -------------------------------\n",
        "counts = multihot.sum(axis=0)\n",
        "\n",
        "print(\"\\n==============================\")\n",
        "print(\" LABEL COUNTS (MULTI-LABEL)\")\n",
        "print(\"==============================\")\n",
        "\n",
        "for lbl, cnt in zip(TARGETS, counts):\n",
        "    print(f\"{lbl:5s} ‚Üí {int(cnt)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Count multi-label combinations\n",
        "# -------------------------------\n",
        "df[\"num_labels\"] = multihot.sum(axis=1)\n",
        "\n",
        "multi_2 = (df[\"num_labels\"] == 2).sum()\n",
        "multi_3 = (df[\"num_labels\"] == 3).sum()\n",
        "multi_4 = (df[\"num_labels\"] == 4).sum()\n",
        "multi_5 = (df[\"num_labels\"] == 5).sum()\n",
        "\n",
        "print(\"\\n==============================\")\n",
        "print(\" MULTI-LABEL DISTRIBUTION\")\n",
        "print(\"==============================\")\n",
        "print(\"2-label samples:\", multi_2)\n",
        "print(\"3-label samples:\", multi_3)\n",
        "print(\"4-label samples:\", multi_4)\n",
        "print(\"5-label samples:\", multi_5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "tytUQlcUx-iu",
        "outputId": "d15d2189-e9f0-4160-c655-5d6c5e16c721"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy<2.0\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "2eaab2616e4847cf872214dddc0c512c",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install \"numpy<2.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmc5J23q4Az2",
        "outputId": "9a3299e9-f47f-4bf2-90af-2c843cb269cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total usable ECGs: 21388\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21388/21388 [3:38:33<00:00,  1.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================\n",
            "‚úÖ DATASET CREATION COMPLETE\n",
            "===================================\n",
            "Saved samples: 21388\n",
            "Errors skipped: 0\n",
            "Dataset path: /content/drive/MyDrive/ECG_ML_MULTI\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# PTB-XL ‚Üí CLEAN MULTI-LABEL ECG DATASET CREATION\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wfdb\n",
        "from scipy.signal import resample_poly\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# CONFIG\n",
        "# ============================================================\n",
        "PTB_ROOT = \"/content/drive/MyDrive/PTB Dataset\"\n",
        "OUT_ROOT = \"/content/drive/MyDrive/ECG_ML_MULTI\"\n",
        "\n",
        "SIGNALS_DIR = f\"{OUT_ROOT}/signals\"\n",
        "LABELS_DIR  = f\"{OUT_ROOT}/labels\"\n",
        "\n",
        "os.makedirs(SIGNALS_DIR, exist_ok=True)\n",
        "os.makedirs(LABELS_DIR, exist_ok=True)\n",
        "\n",
        "TARGET_FS = 100\n",
        "TARGET_LEN = 1000\n",
        "\n",
        "LABELS = [\"NORM\", \"MI\", \"HYP\", \"STTC\", \"CD\"]\n",
        "LABEL_MAP = {l: i for i, l in enumerate(LABELS)}\n",
        "\n",
        "# ============================================================\n",
        "# LOAD METADATA\n",
        "# ============================================================\n",
        "df = pd.read_csv(f\"{PTB_ROOT}/ptbxl_database.csv\")\n",
        "scp = pd.read_csv(f\"{PTB_ROOT}/scp_statements.csv\", index_col=0)\n",
        "\n",
        "def extract_labels(scp_codes):\n",
        "    codes = eval(scp_codes)\n",
        "    out = set()\n",
        "    for c in codes:\n",
        "        if c in scp.index:\n",
        "            cls = scp.loc[c][\"diagnostic_class\"]\n",
        "            if cls in LABEL_MAP:\n",
        "                out.add(cls)\n",
        "    return sorted(list(out))\n",
        "\n",
        "df[\"labels\"] = df[\"scp_codes\"].apply(extract_labels)\n",
        "\n",
        "# Keep samples with at least one target label\n",
        "df = df[df[\"labels\"].map(len) > 0].reset_index(drop=True)\n",
        "\n",
        "print(f\"Total usable ECGs: {len(df)}\")\n",
        "\n",
        "# ============================================================\n",
        "# SIGNAL LOADER\n",
        "# ============================================================\n",
        "def load_and_process(record_path):\n",
        "    sig, fields = wfdb.rdsamp(record_path)\n",
        "    fs = fields[\"fs\"]\n",
        "\n",
        "    # Resample to 100 Hz\n",
        "    if fs != TARGET_FS:\n",
        "        sig = resample_poly(sig, TARGET_FS, fs, axis=0)\n",
        "\n",
        "    # Fix length\n",
        "    if sig.shape[0] < TARGET_LEN:\n",
        "        pad = TARGET_LEN - sig.shape[0]\n",
        "        sig = np.pad(sig, ((0, pad), (0, 0)), mode=\"edge\")\n",
        "    else:\n",
        "        sig = sig[:TARGET_LEN]\n",
        "\n",
        "    return sig.astype(np.float32)\n",
        "\n",
        "# ============================================================\n",
        "# DATASET CREATION\n",
        "# ============================================================\n",
        "meta_rows = []\n",
        "idx = 0\n",
        "errors = 0\n",
        "\n",
        "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    try:\n",
        "        record_path = f\"{PTB_ROOT}/{row['filename_lr']}\"\n",
        "        signal = load_and_process(record_path)\n",
        "\n",
        "        if signal.shape != (TARGET_LEN, 12):\n",
        "            raise ValueError(\"Bad signal shape\")\n",
        "\n",
        "        label_vec = np.zeros(len(LABELS), dtype=np.float32)\n",
        "        for l in row[\"labels\"]:\n",
        "            label_vec[LABEL_MAP[l]] = 1.0\n",
        "\n",
        "        np.save(f\"{SIGNALS_DIR}/{idx}.npy\", signal)\n",
        "        np.save(f\"{LABELS_DIR}/{idx}.npy\", label_vec)\n",
        "\n",
        "        meta_rows.append({\n",
        "            \"id\": idx,\n",
        "            \"ecg_id\": row[\"ecg_id\"],\n",
        "            \"labels\": \",\".join(row[\"labels\"])\n",
        "        })\n",
        "\n",
        "        idx += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        errors += 1\n",
        "        continue\n",
        "\n",
        "# ============================================================\n",
        "# SAVE METADATA\n",
        "# ============================================================\n",
        "meta_df = pd.DataFrame(meta_rows)\n",
        "meta_df.to_csv(f\"{OUT_ROOT}/metadata.csv\", index=False)\n",
        "\n",
        "print(\"\\n===================================\")\n",
        "print(\"‚úÖ DATASET CREATION COMPLETE\")\n",
        "print(\"===================================\")\n",
        "print(f\"Saved samples: {idx}\")\n",
        "print(f\"Errors skipped: {errors}\")\n",
        "print(f\"Dataset path: {OUT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbV8RWsGNWs_",
        "outputId": "b6ab37bc-a617-42b4-8fd0-cb16ac7ea498"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Scanning label files...\n",
            "Found 21388 label files\n",
            "‚ö° Loading labels in parallel...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21388/21388 [04:14<00:00, 83.89it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Labels loaded\n",
            "Shape: (21388, 5)\n",
            "üéâ all_labels_summary.npy saved to:\n",
            "/content/drive/MyDrive/ECG_ML_MULTI/all_labels_summary.npy\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CREATE all_labels_summary.npy (SAFE + FAST)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "\n",
        "DATA_ROOT = \"/content/drive/MyDrive/ECG_ML_MULTI\"\n",
        "LABELS_DIR = f\"{DATA_ROOT}/labels\"\n",
        "SUMMARY_PATH = f\"{DATA_ROOT}/all_labels_summary.npy\"\n",
        "\n",
        "print(\"üîç Scanning label files...\")\n",
        "ids = sorted([int(f.split(\".\")[0]) for f in os.listdir(LABELS_DIR)])\n",
        "\n",
        "print(f\"Found {len(ids)} label files\")\n",
        "\n",
        "def load_label(fid):\n",
        "    try:\n",
        "        return np.load(f\"{LABELS_DIR}/{fid}.npy\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load label {fid}: {e}\")\n",
        "        return None\n",
        "\n",
        "labels = []\n",
        "\n",
        "print(\"‚ö° Loading labels in parallel...\")\n",
        "with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "    for lbl in tqdm(executor.map(load_label, ids), total=len(ids)):\n",
        "        if lbl is not None:\n",
        "            labels.append(lbl)\n",
        "\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(\"‚úÖ Labels loaded\")\n",
        "print(\"Shape:\", labels.shape)\n",
        "\n",
        "# Save summary\n",
        "np.save(SUMMARY_PATH, labels)\n",
        "print(f\"üéâ all_labels_summary.npy saved to:\\n{SUMMARY_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlrePynTVxOt",
        "outputId": "2b680dc9-27e1-48be-c14b-95c38a515a56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üïµÔ∏è‚Äç‚ôÇÔ∏è Scanning /content/drive/MyDrive/ECG_ML_MULTI/signals...\n",
            "‚ö†Ô∏è This will take 2-5 minutes depending on Google Drive speed.\n",
            "   Please be patient and let it finish!\n",
            "\n",
            "=======================================\n",
            "‚úÖ SUCCESS! Found 21388 files.\n",
            "üíæ Cache saved to: /content/drive/MyDrive/ECG_ML_MULTI/all_ids_cache.npy\n",
            "=======================================\n",
            "üëâ You can now run the MAIN TRAINING script. It will load instantly.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# --- CONFIG ---\n",
        "DATA_ROOT = \"/content/drive/MyDrive/ECG_ML_MULTI\"\n",
        "SIGNALS_DIR = f\"{DATA_ROOT}/signals\"\n",
        "IDS_CACHE_PATH = f\"{DATA_ROOT}/all_ids_cache.npy\"\n",
        "\n",
        "print(f\"üïµÔ∏è‚Äç‚ôÇÔ∏è Scanning {SIGNALS_DIR}...\")\n",
        "print(\"‚ö†Ô∏è This will take 2-5 minutes depending on Google Drive speed.\")\n",
        "print(\"   Please be patient and let it finish!\")\n",
        "\n",
        "# Use scandir (faster than listdir)\n",
        "ids = []\n",
        "try:\n",
        "    with os.scandir(SIGNALS_DIR) as entries:\n",
        "        for entry in entries:\n",
        "            # We only want .npy files\n",
        "            if entry.name.endswith('.npy'):\n",
        "                # Convert \"1023.npy\" -> 1023\n",
        "                try:\n",
        "                    file_id = int(entry.name.split('.')[0])\n",
        "                    ids.append(file_id)\n",
        "                except ValueError:\n",
        "                    pass # Skip non-integer filenames\n",
        "\n",
        "    # Sort them to ensure consistent order\n",
        "    ids = sorted(ids)\n",
        "\n",
        "    # Save to Drive\n",
        "    np.save(IDS_CACHE_PATH, ids)\n",
        "\n",
        "    print(\"\\n=======================================\")\n",
        "    print(f\"‚úÖ SUCCESS! Found {len(ids)} files.\")\n",
        "    print(f\"üíæ Cache saved to: {IDS_CACHE_PATH}\")\n",
        "    print(\"=======================================\")\n",
        "    print(\"üëâ You can now run the MAIN TRAINING script. It will load instantly.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: The folder {SIGNALS_DIR} does not exist. Check your path!\")\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nüõë Process stopped by user.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0gQPfZIhQce",
        "outputId": "86557b99-2ab5-4031-8fc9-68d4e69906d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è Cache not found. Loading 21388 signals into RAM...\n",
            "   (This will take 2-4 minutes. Watch the bar!)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21388/21388 [18:06<00:00, 19.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stacking array...\n",
            "Saving 1.03 GB to Drive...\n",
            "‚úÖ DONE! Signals cached.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "\n",
        "DATA_ROOT = \"/content/drive/MyDrive/ECG_ML_MULTI\"\n",
        "SIGNALS_DIR = f\"{DATA_ROOT}/signals\"\n",
        "CACHE_PATH = f\"{DATA_ROOT}/all_signals_cache.npy\"\n",
        "IDS_PATH = f\"{DATA_ROOT}/all_ids_cache.npy\"\n",
        "\n",
        "# Load the IDs order so signals match labels\n",
        "ids = np.load(IDS_PATH)\n",
        "\n",
        "if os.path.exists(CACHE_PATH):\n",
        "    print(\"‚úÖ Signals cache already exists! You are ready to train.\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Cache not found. Loading {len(ids)} signals into RAM...\")\n",
        "    print(\"   (This will take 2-4 minutes. Watch the bar!)\")\n",
        "\n",
        "    def load_sig(fid):\n",
        "        # Load and transpose to (12, 1000) immediately to save space/time later\n",
        "        arr = np.load(f\"{SIGNALS_DIR}/{fid}.npy\")\n",
        "        return arr.T.astype(np.float32)\n",
        "\n",
        "    # Parallel Load\n",
        "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "        results = list(tqdm(executor.map(load_sig, ids), total=len(ids)))\n",
        "\n",
        "    # Stack into big array (N, 12, 1000)\n",
        "    print(\"Stacking array...\")\n",
        "    X_all = np.stack(results)\n",
        "\n",
        "    # Save\n",
        "    print(f\"Saving {X_all.nbytes / 1e9:.2f} GB to Drive...\")\n",
        "    np.save(CACHE_PATH, X_all)\n",
        "    print(\"‚úÖ DONE! Signals cached.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooTCOvF7yUuI",
        "outputId": "ccdeacb4-c8da-4a26-e32f-e2f01f36e9f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "‚ö° Signals Cache found!\n",
            "‚è≥ Loading caches into RAM...\n",
            "‚úÖ Data Loaded. Shape: (21388, 12, 1000)\n",
            "‚úÇÔ∏è Performing Stratified Split...\n",
            "Train: 17114 | Val: 4274\n",
            "\n",
            "üöÄ STARTING FAST TRAINING...\n",
            "Epoch 01 | Loss: 0.8611 | Val F1: 0.6162\n",
            "Epoch 02 | Loss: 0.6883 | Val F1: 0.6483\n",
            "Epoch 03 | Loss: 0.6392 | Val F1: 0.6537\n",
            "Epoch 04 | Loss: 0.6168 | Val F1: 0.6677\n",
            "Epoch 05 | Loss: 0.5942 | Val F1: 0.6714\n",
            "Epoch 06 | Loss: 0.5770 | Val F1: 0.6820\n",
            "Epoch 07 | Loss: 0.5665 | Val F1: 0.6933\n",
            "Epoch 08 | Loss: 0.5531 | Val F1: 0.6981\n",
            "Epoch 09 | Loss: 0.5476 | Val F1: 0.7135\n",
            "Epoch 10 | Loss: 0.5383 | Val F1: 0.7062\n",
            "Epoch 11 | Loss: 0.5300 | Val F1: 0.7116\n",
            "Epoch 12 | Loss: 0.5277 | Val F1: 0.7022\n",
            "Epoch 13 | Loss: 0.5205 | Val F1: 0.7054\n",
            "Epoch 14 | Loss: 0.5152 | Val F1: 0.7076\n",
            "Epoch 15 | Loss: 0.5075 | Val F1: 0.7133\n",
            "Epoch 16 | Loss: 0.5010 | Val F1: 0.7141\n",
            "Epoch 17 | Loss: 0.4941 | Val F1: 0.7177\n",
            "Epoch 18 | Loss: 0.4907 | Val F1: 0.6985\n",
            "Epoch 19 | Loss: 0.4880 | Val F1: 0.7079\n",
            "Epoch 20 | Loss: 0.4801 | Val F1: 0.7113\n",
            "Epoch 21 | Loss: 0.4728 | Val F1: 0.7105\n",
            "Epoch 22 | Loss: 0.4697 | Val F1: 0.7281\n",
            "Epoch 23 | Loss: 0.4665 | Val F1: 0.7305\n",
            "Epoch 24 | Loss: 0.4579 | Val F1: 0.7207\n",
            "Epoch 25 | Loss: 0.4511 | Val F1: 0.7144\n",
            "Epoch 26 | Loss: 0.4479 | Val F1: 0.7230\n",
            "Epoch 27 | Loss: 0.4413 | Val F1: 0.7223\n",
            "Epoch 28 | Loss: 0.4353 | Val F1: 0.7156\n",
            "Epoch 29 | Loss: 0.4289 | Val F1: 0.7272\n",
            "Epoch 30 | Loss: 0.4231 | Val F1: 0.7133\n",
            "\n",
            "üìä FINAL RESULTS\n",
            "\n",
            "NORM (Thresh 0.54)\n",
            "F1: 0.870\n",
            "AUC: 0.950\n",
            "[[2056  315]\n",
            " [ 194 1709]]\n",
            "\n",
            "MI (Thresh 0.58)\n",
            "F1: 0.790\n",
            "AUC: 0.945\n",
            "[[2896  284]\n",
            " [ 194  900]]\n",
            "\n",
            "HYP (Thresh 0.72)\n",
            "F1: 0.547\n",
            "AUC: 0.864\n",
            "[[3469  275]\n",
            " [ 227  303]]\n",
            "\n",
            "STTC (Thresh 0.57)\n",
            "F1: 0.762\n",
            "AUC: 0.933\n",
            "[[2909  318]\n",
            " [ 207  840]]\n",
            "\n",
            "CD (Thresh 0.58)\n",
            "F1: 0.777\n",
            "AUC: 0.928\n",
            "[[3106  188]\n",
            " [ 238  742]]\n",
            "\n",
            "‚úÖ Done. Best Macro F1: 0.731\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# FINAL COMPLETE ECG PIPELINE\n",
        "# (RAM Optimized + Augmentation + Stratification + Thresholds)\n",
        "# ============================================================\n",
        "\n",
        "# 1. INSTALL MISSING DEPENDENCIES\n",
        "import subprocess\n",
        "import sys\n",
        "def install(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "try:\n",
        "    import iterstrat\n",
        "except ImportError:\n",
        "    print(\"üì¶ Installing iterative-stratification...\")\n",
        "    install(\"iterative-stratification\")\n",
        "\n",
        "# 2. IMPORTS\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# ---------------- CONFIGURATION ----------------\n",
        "DATA_ROOT = \"/content/drive/MyDrive/ECG_ML_MULTI\"\n",
        "SIGNALS_DIR = f\"{DATA_ROOT}/signals\"\n",
        "LABELS_DIR  = f\"{DATA_ROOT}/labels\"\n",
        "\n",
        "# Cache Files\n",
        "SIGNALS_CACHE = f\"{DATA_ROOT}/all_signals_cache.npy\"\n",
        "LABELS_CACHE  = f\"{DATA_ROOT}/all_labels_summary.npy\"\n",
        "IDS_CACHE     = f\"{DATA_ROOT}/all_ids_cache.npy\"\n",
        "\n",
        "SAVE_MODEL_PATH = \"/content/drive/MyDrive/best_ecg_model.pt\"\n",
        "SAVE_THRESH_PATH = \"/content/drive/MyDrive/best_thresholds.npy\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 64 # Larger batch size for RAM speed\n",
        "EPOCHS = 30\n",
        "LR = 1e-3\n",
        "LEADS = 12\n",
        "CLASSES = 5\n",
        "CLASS_NAMES = [\"NORM\", \"MI\", \"HYP\", \"STTC\", \"CD\"]\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# ---------------- 3. CACHE GENERATION (AUTO) ----------------\n",
        "# This block ensures the big RAM files exist. If not, it creates them.\n",
        "def generate_caches_if_missing():\n",
        "    # A. Check/Generate IDs\n",
        "    if not os.path.exists(IDS_CACHE):\n",
        "        print(\"‚ö†Ô∏è ID Cache missing. Scanning Drive...\")\n",
        "        ids = []\n",
        "        with os.scandir(SIGNALS_DIR) as entries:\n",
        "            for entry in entries:\n",
        "                if entry.name.endswith('.npy'):\n",
        "                    ids.append(int(entry.name.split('.')[0]))\n",
        "        ids = sorted(ids)\n",
        "        np.save(IDS_CACHE, ids)\n",
        "        print(\"‚úÖ IDs Cached.\")\n",
        "    else:\n",
        "        ids = np.load(IDS_CACHE)\n",
        "\n",
        "    # B. Check/Generate Labels\n",
        "    if not os.path.exists(LABELS_CACHE):\n",
        "        print(\"‚ö†Ô∏è Label Cache missing. Loading...\")\n",
        "        def load_lbl(i): return np.load(f\"{LABELS_DIR}/{i}.npy\")\n",
        "        with ThreadPoolExecutor(max_workers=8) as ex:\n",
        "            Y_all = np.array(list(tqdm(ex.map(load_lbl, ids), total=len(ids))))\n",
        "        np.save(LABELS_CACHE, Y_all)\n",
        "        print(\"‚úÖ Labels Cached.\")\n",
        "\n",
        "    # C. Check/Generate Signals (The Big One)\n",
        "    if not os.path.exists(SIGNALS_CACHE):\n",
        "        print(\"‚ö†Ô∏è Signal Cache missing. Loading ALL signals to RAM (2-5 mins)...\")\n",
        "        def load_sig(fid):\n",
        "            # Load and transpose to (12, 1000) immediately\n",
        "            arr = np.load(f\"{SIGNALS_DIR}/{fid}.npy\")\n",
        "            return arr.T.astype(np.float32)\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=8) as ex:\n",
        "            X_all = np.array(list(tqdm(ex.map(load_sig, ids), total=len(ids))))\n",
        "\n",
        "        print(f\"Saving {X_all.nbytes/1e9:.2f} GB cache file...\")\n",
        "        np.save(SIGNALS_CACHE, X_all)\n",
        "        print(\"‚úÖ Signals Cached.\")\n",
        "    else:\n",
        "        print(\"‚ö° Signals Cache found!\")\n",
        "\n",
        "generate_caches_if_missing()\n",
        "\n",
        "# ---------------- 4. RAM DATASET (WITH AUGMENTATION) ----------------\n",
        "class RAMDataset(Dataset):\n",
        "    def __init__(self, X, Y, augment=False):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Y)\n",
        "\n",
        "    def augment_signal(self, x):\n",
        "        # x shape: (12, 1000) - PyTorch Tensor or Numpy\n",
        "        if random.random() < 0.3: # Gaussian Noise\n",
        "            noise = np.random.normal(0, 0.05, x.shape)\n",
        "            x = x + noise\n",
        "        if random.random() < 0.3: # Scale\n",
        "            factor = np.random.uniform(0.9, 1.1)\n",
        "            x = x * factor\n",
        "        if random.random() < 0.2: # Time Shift\n",
        "            shift = np.random.randint(-20, 20)\n",
        "            x = np.roll(x, shift, axis=1)\n",
        "        return x\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx].copy() # Copy to avoid modifying cache\n",
        "        y = self.Y[idx]\n",
        "\n",
        "        if self.augment:\n",
        "            x = self.augment_signal(x)\n",
        "\n",
        "        # Normalize\n",
        "        x = (x - x.mean()) / (x.std() + 1e-6)\n",
        "\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# ---------------- 5. MODEL ----------------\n",
        "class CNN_GraphSAGE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, 7, padding=3), nn.ReLU(),\n",
        "            nn.Conv1d(32, 64, 5, stride=2, padding=2), nn.ReLU(),\n",
        "            nn.Conv1d(64, 64, 5, stride=2, padding=2), nn.ReLU()\n",
        "        )\n",
        "        edges = [(0,1),(1,2),(0,2),(3,4),(4,5),(3,5),(5,6),(6,7),(5,7),(7,8),(8,9),(7,9),(9,10),(10,11),(9,11)]\n",
        "        self.register_buffer(\"edge_index\", torch.tensor(edges, dtype=torch.long).t())\n",
        "        self.sage1 = SAGEConv(64, 64)\n",
        "        self.sage2 = SAGEConv(64, 64)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64 * LEADS, 256), nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(256, CLASSES)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, T = x.shape\n",
        "        x = x.reshape(B * L, 1, T)\n",
        "        feat = self.cnn(x).mean(dim=-1)\n",
        "        nodes = feat.reshape(B, L, 64)\n",
        "        flat = nodes.reshape(B * L, 64)\n",
        "        edge_list = [self.edge_index + i*L for i in range(B)]\n",
        "        edges = torch.cat(edge_list, dim=1).to(flat.device)\n",
        "        g = F.relu(self.sage1(flat, edges))\n",
        "        g = self.sage2(g, edges)\n",
        "        g = g.reshape(B, -1)\n",
        "        return self.fc(g)\n",
        "\n",
        "# ---------------- 6. SETUP & TRAINING ----------------\n",
        "\n",
        "# Load Data to RAM\n",
        "print(\"‚è≥ Loading caches into RAM...\")\n",
        "X_all = np.load(SIGNALS_CACHE)\n",
        "Y_all = np.load(LABELS_CACHE)\n",
        "ids   = np.load(IDS_CACHE)\n",
        "print(f\"‚úÖ Data Loaded. Shape: {X_all.shape}\")\n",
        "\n",
        "# Stratified Split\n",
        "print(\"‚úÇÔ∏è Performing Stratified Split...\")\n",
        "mss = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# We only take the first fold for training/val split\n",
        "train_indices, val_indices = next(mss.split(ids, Y_all))\n",
        "\n",
        "print(f\"Train: {len(train_indices)} | Val: {len(val_indices)}\")\n",
        "\n",
        "train_ds = RAMDataset(X_all[train_indices], Y_all[train_indices], augment=True)\n",
        "val_ds   = RAMDataset(X_all[val_indices],   Y_all[val_indices],   augment=False)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Class Weights\n",
        "pos = Y_all[train_indices].sum(axis=0)\n",
        "pos_weight = torch.tensor((len(train_indices) - pos) / (pos + 1e-6), dtype=torch.float32).to(DEVICE)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "model = CNN_GraphSAGE().to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "print(\"\\nüöÄ STARTING FAST TRAINING...\")\n",
        "best_f1 = 0\n",
        "best_logits, best_labels = None, None\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    logits_list, labels_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x = x.to(DEVICE)\n",
        "            logits_list.append(model(x).cpu().numpy())\n",
        "            labels_list.append(y.numpy())\n",
        "\n",
        "    logits_arr = np.vstack(logits_list)\n",
        "    labels_arr = np.vstack(labels_list)\n",
        "    probs = torch.sigmoid(torch.tensor(logits_arr)).numpy()\n",
        "\n",
        "    f1 = f1_score(labels_arr, probs > 0.5, average=\"macro\")\n",
        "    print(f\"Epoch {epoch:02d} | Loss: {train_loss/len(train_loader):.4f} | Val F1: {f1:.4f}\")\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_logits = probs\n",
        "        best_labels = labels_arr\n",
        "        torch.save(model.state_dict(), SAVE_MODEL_PATH)\n",
        "\n",
        "# ---------------- 7. EVALUATION ----------------\n",
        "print(\"\\nüìä FINAL RESULTS\")\n",
        "best_thresholds = []\n",
        "for c in range(CLASSES):\n",
        "    best_t, best_score = 0.5, 0\n",
        "    for t in np.linspace(0.1, 0.9, 81):\n",
        "        score = f1_score(best_labels[:,c], best_logits[:,c] > t)\n",
        "        if score > best_score: best_score, best_t = score, t\n",
        "    best_thresholds.append(best_t)\n",
        "\n",
        "    preds = best_logits[:,c] > best_t\n",
        "    print(f\"\\n{CLASS_NAMES[c]} (Thresh {best_t:.2f})\")\n",
        "    print(f\"F1: {f1_score(best_labels[:,c], preds):.3f}\")\n",
        "    print(f\"AUC: {roc_auc_score(best_labels[:,c], best_logits[:,c]):.3f}\")\n",
        "    print(confusion_matrix(best_labels[:,c], preds))\n",
        "\n",
        "np.save(SAVE_THRESH_PATH, np.array(best_thresholds))\n",
        "print(f\"\\n‚úÖ Done. Best Macro F1: {best_f1:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FxFcYvdv29O",
        "outputId": "363ca98b-56c2-45d0-dfc0-e0573e44f42c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1086705892.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  binary_model.load_state_dict(torch.load(BINARY_MODEL_PATH, map_location=DEVICE))\n",
            "/tmp/ipython-input-1086705892.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  multi_model.load_state_dict(torch.load(MULTI_MODEL_PATH, map_location=DEVICE))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'stage': 'BINARY', 'prediction': 'NORMAL', 'confidence': 0.0, 'explanation': 'ECG morphology consistent with normal patterns'}\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# FINAL ECG AI PIPELINE (FIXED)\n",
        "# Image ‚Üí Binary ResNet18 ‚Üí Digitizer ‚Üí CNN + GraphSAGE\n",
        "# ============================================================\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "# ============================================================\n",
        "# CONFIG\n",
        "# ============================================================\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "BINARY_MODEL_PATH = \"/content/drive/MyDrive/best_binary_resnet18_1.pt\"\n",
        "MULTI_MODEL_PATH  = \"/content/drive/MyDrive/best_ecg_model.pt\"\n",
        "MULTI_THRESH_PATH = \"/content/drive/MyDrive/best_thresholds.npy\"\n",
        "\n",
        "ABNORMAL_THRESHOLD = 0.40    # probability threshold\n",
        "CLASS_NAMES = [\"NORM\", \"MI\", \"HYP\", \"STTC\", \"CD\"]\n",
        "\n",
        "# ============================================================\n",
        "# LOAD BINARY RESNET18 MODEL\n",
        "# ============================================================\n",
        "\n",
        "binary_model = models.resnet18(weights=None)\n",
        "binary_model.fc = nn.Linear(binary_model.fc.in_features, 1)\n",
        "\n",
        "binary_model.load_state_dict(torch.load(BINARY_MODEL_PATH, map_location=DEVICE))\n",
        "binary_model.to(DEVICE)\n",
        "binary_model.eval()\n",
        "\n",
        "binary_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485,0.456,0.406],\n",
        "        std =[0.229,0.224,0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# ============================================================\n",
        "# DIGITIZER ‚Äî Converts ECG Image ‚Üí 12-lead (1000√ó12) signal\n",
        "# ============================================================\n",
        "\n",
        "def digitize_ecg_image(img_path, target_len=1000, leads=12):\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        raise ValueError(\"Invalid image path\")\n",
        "\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    gray = cv2.GaussianBlur(gray, (5,5), 0)\n",
        "\n",
        "    if gray.mean() > 127:\n",
        "        gray = 255 - gray\n",
        "\n",
        "    th = cv2.adaptiveThreshold(\n",
        "        gray, 255,\n",
        "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "        cv2.THRESH_BINARY,\n",
        "        11, 2\n",
        "    )\n",
        "\n",
        "    # remove ECG grid lines\n",
        "    kernel_h = cv2.getStructuringElement(cv2.MORPH_RECT, (25,1))\n",
        "    kernel_v = cv2.getStructuringElement(cv2.MORPH_RECT, (1,25))\n",
        "\n",
        "    grid = cv2.morphologyEx(th, cv2.MORPH_OPEN, kernel_h)\n",
        "    grid += cv2.morphologyEx(th, cv2.MORPH_OPEN, kernel_v)\n",
        "\n",
        "    wave = cv2.subtract(th, grid)\n",
        "\n",
        "    h, w = wave.shape\n",
        "    lead_h = h // leads\n",
        "    signals = []\n",
        "\n",
        "    for i in range(leads):\n",
        "        lead = wave[i*lead_h:(i+1)*lead_h, :]\n",
        "        col_sum = np.sum(255 - lead, axis=0)\n",
        "\n",
        "        if col_sum.max() == 0:\n",
        "            sig = np.zeros(target_len)\n",
        "        else:\n",
        "            sig = col_sum / (col_sum.max() + 1e-6)\n",
        "            sig = np.interp(\n",
        "                np.linspace(0, len(sig), target_len),\n",
        "                np.arange(len(sig)),\n",
        "                sig\n",
        "            )\n",
        "\n",
        "        signals.append(sig)\n",
        "\n",
        "    signal = np.stack(signals, axis=1)\n",
        "    signal = (signal - signal.mean()) / (signal.std() + 1e-6)\n",
        "\n",
        "    return signal.astype(np.float32)\n",
        "\n",
        "# ============================================================\n",
        "# CNN + GRAPH SAGE MODEL\n",
        "# ============================================================\n",
        "\n",
        "class CNN_GraphSAGE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, 7, padding=3), nn.ReLU(),\n",
        "            nn.Conv1d(32, 64, 5, stride=2, padding=2), nn.ReLU(),\n",
        "            nn.Conv1d(64, 64, 5, stride=2, padding=2), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        edges = [\n",
        "            (0,1),(1,2),(0,2),\n",
        "            (3,4),(4,5),(3,5),\n",
        "            (5,6),(6,7),(5,7),\n",
        "            (7,8),(8,9),(7,9),\n",
        "            (9,10),(10,11),(9,11)\n",
        "        ]\n",
        "        self.register_buffer(\"edge_index\", torch.tensor(edges, dtype=torch.long).t())\n",
        "\n",
        "        self.sage1 = SAGEConv(64, 64)\n",
        "        self.sage2 = SAGEConv(64, 64)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64*12, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 5)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, T = x.shape\n",
        "        x = x.reshape(B*L, 1, T)\n",
        "        feat = self.cnn(x).mean(-1)\n",
        "\n",
        "        nodes = feat.reshape(B, L, 64)\n",
        "        flat  = nodes.reshape(B*L, 64)\n",
        "\n",
        "        edges = torch.cat([self.edge_index + i*L for i in range(B)], dim=1).to(flat.device)\n",
        "\n",
        "        g = F.relu(self.sage1(flat, edges))\n",
        "        g = self.sage2(g, edges)\n",
        "        g = g.reshape(B, -1)\n",
        "\n",
        "        return self.fc(g)\n",
        "\n",
        "multi_model = CNN_GraphSAGE().to(DEVICE)\n",
        "multi_model.load_state_dict(torch.load(MULTI_MODEL_PATH, map_location=DEVICE))\n",
        "multi_model.eval()\n",
        "\n",
        "multi_thresholds = np.load(MULTI_THRESH_PATH)\n",
        "\n",
        "# ============================================================\n",
        "# FINAL PREDICT FUNCTION\n",
        "# ============================================================\n",
        "\n",
        "def predict_ecg(img_path):\n",
        "\n",
        "    # ------------------------------\n",
        "    # BINARY NORMAL vs ABNORMAL STAGE\n",
        "    # ------------------------------\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    x_img = binary_transform(img).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        p_abnormal = torch.sigmoid(binary_model(x_img)).item()\n",
        "\n",
        "    # NORMAL ECG ‚Üí STOP PIPELINE\n",
        "    if p_abnormal < ABNORMAL_THRESHOLD:\n",
        "        return {\n",
        "            \"stage\": \"BINARY\",\n",
        "            \"predicted_class\": \"NORMAL\",\n",
        "            \"confidence\": round(1 - p_abnormal, 3),\n",
        "            \"risk_level\": \"low\",\n",
        "            \"risk_score\": int((1 - p_abnormal) * 100),\n",
        "            \"probabilities\": {},\n",
        "            \"explanation\": \"Binary classifier detected normal ECG pattern\"\n",
        "        }\n",
        "\n",
        "    # ABNORMAL ECG ‚Üí Continue to multi-label analysis\n",
        "    # ------------------------------\n",
        "    # DIGITIZE ECG SIGNAL\n",
        "    # ------------------------------\n",
        "    signal = digitize_ecg_image(img_path)\n",
        "    x_sig = torch.tensor(signal.T).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    # ------------------------------\n",
        "    # MULTI-LABEL CLASSIFICATION\n",
        "    # ------------------------------\n",
        "    with torch.no_grad():\n",
        "        logits = multi_model(x_sig)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
        "\n",
        "    # Apply class thresholds\n",
        "    labels = [\n",
        "        CLASS_NAMES[i]\n",
        "        for i in range(5)\n",
        "        if probs[i] >= multi_thresholds[i]\n",
        "    ]\n",
        "\n",
        "    if not labels:\n",
        "        labels = [\"ABNORMAL\"]\n",
        "\n",
        "    return {\n",
        "        \"stage\": \"MULTI\",\n",
        "        \"predicted_class\": labels[0],\n",
        "        \"confidence\": round(float(probs[CLASS_NAMES.index(labels[0])]) * 100, 1),\n",
        "        \"risk_level\": \"high\" if labels[0] in [\"MI\", \"STTC\"] else \"moderate\",\n",
        "        \"risk_score\": int(float(probs[CLASS_NAMES.index(labels[0])]) * 100),\n",
        "        \"probabilities\": {\n",
        "            CLASS_NAMES[i]: round(float(probs[i]), 3)\n",
        "            for i in range(5)\n",
        "        },\n",
        "        \"explanation\": \"Abnormal ECG detected ‚Üí Multi-label CNN+GraphSAGE classifier used\"\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TEST\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(predict_ecg(\"/content/drive/MyDrive/Normal(65).jpg\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPEd14burrh3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
